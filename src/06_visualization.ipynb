{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Comparison\n",
    "\n",
    "This notebook provides comprehensive visualizations comparing all models, including hybrid architectures, with focus on:\n",
    "1. Model accuracy\n",
    "2. Model size and efficiency\n",
    "3. Optimization results\n",
    "4. Performance trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load all results\n",
    "with open('results/training_results.pkl', 'rb') as f:\n",
    "    training_results = pickle.load(f)\n",
    "\n",
    "with open('results/optimization_results.pkl', 'rb') as f:\n",
    "    optimization_results = pickle.load(f)\n",
    "\n",
    "with open('results/regression_metrics.pkl', 'rb') as f:\n",
    "    regression_metrics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_comparison_df():\n",
    "    \"\"\"Create DataFrame with model comparisons\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for name, result in training_results.items():\n",
    "        model_data = {\n",
    "            'Model': name,\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'Parameters': result['model'].count_params(),\n",
    "            'Original Size (KB)': optimization_results[name]['Original']['size'],\n",
    "            'Optimized Size (KB)': optimization_results[name]['Quantized']['size'],\n",
    "            'Size Reduction (%)': ((optimization_results[name]['Original']['size'] - \n",
    "                                   optimization_results[name]['Quantized']['size']) / \n",
    "                                   optimization_results[name]['Original']['size'] * 100)\n",
    "        }\n",
    "        data.append(model_data)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "comparison_df = create_comparison_df()\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Plot accuracy vs size\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(comparison_df['Optimized Size (KB)'], comparison_df['Accuracy'], \n",
    "           alpha=0.6, s=100)\n",
    "\n",
    "for i, model in enumerate(comparison_df['Model']):\n",
    "    plt.annotate(model, \n",
    "                 (comparison_df['Optimized Size (KB)'].iloc[i],\n",
    "                  comparison_df['Accuracy'].iloc[i]))\n",
    "\n",
    "plt.title('Model Accuracy vs Size (After Optimization)')\n",
    "plt.xlabel('Model Size (KB)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot optimization stages for each model\n",
    "stages = ['Original', 'Architecture Optimized', 'Compressed', 'Quantized']\n",
    "models = list(optimization_results.keys())\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Size reduction plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for model in models:\n",
    "    sizes = [optimization_results[model][stage]['size'] for stage in stages]\n",
    "    plt.plot(stages, sizes, marker='o', label=model)\n",
    "\n",
    "plt.title('Model Size Through Optimization Stages')\n",
    "plt.xlabel('Optimization Stage')\n",
    "plt.ylabel('Size (KB)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Accuracy preservation plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for model in models:\n",
    "    accuracies = [optimization_results[model][stage]['accuracy'] for stage in stages]\n",
    "    plt.plot(stages, accuracies, marker='o', label=model)\n",
    "\n",
    "plt.title('Model Accuracy Through Optimization Stages')\n",
    "plt.xlabel('Optimization Stage')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hybrid Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Focus on hybrid models\n",
    "hybrid_models = ['CNN-LSTM', 'CNN-BiLSTM']\n",
    "original_models = ['1DCNN', 'LSTM', 'BiLSTM']\n",
    "\n",
    "# Compare with original architectures\n",
    "hybrid_comparison = comparison_df[\n",
    "    comparison_df['Model'].isin(hybrid_models + original_models)\n",
    "].copy()\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Size vs Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for model_type, marker, size in zip(['Original', 'Hybrid'],\n",
    "                                    ['o', 's'],\n",
    "                                    [100, 150]):\n",
    "    mask = hybrid_comparison['Model'].isin(hybrid_models if model_type == 'Hybrid' else original_models)\n",
    "    plt.scatter(hybrid_comparison[mask]['Optimized Size (KB)'],\n",
    "               hybrid_comparison[mask]['Accuracy'],\n",
    "               label=model_type,\n",
    "               marker=marker,\n",
    "               s=size,\n",
    "               alpha=0.6)\n",
    "\n",
    "for i, row in hybrid_comparison.iterrows():\n",
    "    plt.annotate(row['Model'],\n",
    "                 (row['Optimized Size (KB)'], row['Accuracy']))\n",
    "\n",
    "plt.title('Hybrid vs Original Models')\n",
    "plt.xlabel('Optimized Size (KB)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Size reduction comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "bars = plt.bar(hybrid_comparison['Model'],\n",
    "               hybrid_comparison['Size Reduction (%)'])\n",
    "\n",
    "# Color bars by model type\n",
    "for i, bar in enumerate(bars):\n",
    "    if hybrid_comparison['Model'].iloc[i] in hybrid_models:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('blue')\n",
    "\n",
    "plt.title('Size Reduction After Optimization')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Size Reduction (%)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Trade-off Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_efficiency_score(row):\n",
    "    \"\"\"Calculate efficiency score (accuracy/size ratio)\"\"\"\n",
    "    return row['Accuracy'] / row['Optimized Size (KB)'] * 1000  # Scale for readability\n",
    "\n",
    "# Add efficiency score\n",
    "comparison_df['Efficiency Score'] = comparison_df.apply(calculate_efficiency_score, axis=1)\n",
    "\n",
    "# Plot efficiency scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(comparison_df['Model'], comparison_df['Efficiency Score'])\n",
    "\n",
    "# Color bars by model type\n",
    "for i, bar in enumerate(bars):\n",
    "    if comparison_df['Model'].iloc[i] in hybrid_models:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('blue')\n",
    "\n",
    "plt.title('Model Efficiency (Accuracy/Size Ratio)')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Efficiency Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(comparison_df['Efficiency Score']):\n",
    "    plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nModel Efficiency Summary:\")\n",
    "print(comparison_df[['Model', 'Accuracy', 'Optimized Size (KB)', 'Efficiency Score']]\n",
    "      .sort_values('Efficiency Score', ascending=False)\n",
    "      .to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv('results/model_comparison.csv', index=False)\n",
    "\n",
    "# Create and save final summary\n",
    "final_summary = {\n",
    "    'best_model': comparison_df.loc[comparison_df['Efficiency Score'].idxmax(), 'Model'],\n",
    "    'hybrid_performance': {\n",
    "        model: {\n",
    "            'accuracy': comparison_df[comparison_df['Model'] == model]['Accuracy'].iloc[0],\n",
    "            'size': comparison_df[comparison_df['Model'] == model]['Optimized Size (KB)'].iloc[0],\n",
    "            'efficiency': comparison_df[comparison_df['Model'] == model]['Efficiency Score'].iloc[0]\n",
    "        }\n",
    "        for model in hybrid_models\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('results/final_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(final_summary, f)\n",
    "\n",
    "print(\"\\nResults saved successfully!\")\n",
    "\n",
    "# Print final recommendation\n",
    "print(\"\\nFinal Recommendation:\")\n",
    "print(f\"Best performing model (efficiency-wise): {final_summary['best_model']}\")\n",
    "print(\"\\nHybrid Model Performance:\")\n",
    "for model, metrics in final_summary['hybrid_performance'].items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Optimized Size: {metrics['size']:.2f} KB\")\n",
    "    print(f\"Efficiency Score: {metrics['efficiency']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}