{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Metrics\n",
    "\n",
    "This notebook analyzes model performance with focus on efficiency metrics:\n",
    "1. Memory usage\n",
    "2. Inference time\n",
    "3. Model size\n",
    "4. Efficiency scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import (\n",
    "    measure_model_metrics,\n",
    "    calculate_efficiency_score,\n",
    "    get_model_summary,\n",
    "    optimize_batch_size\n",
    ")\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load test data\n",
    "X_test = np.load('preprocessed/X_test.npy')\n",
    "y_test = np.load('preprocessed/y_test.npy')\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Load all models\n",
    "models = {}\n",
    "for model_file in os.listdir('models'):\n",
    "    if model_file.endswith('.h5'):\n",
    "        name = model_file[:-3]\n",
    "        models[name] = tf.keras.models.load_model(f'models/{model_file}')\n",
    "\n",
    "print(f\"Loaded {len(models)} models:\")\n",
    "for name in models.keys():\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measure Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Measure metrics for each model\n",
    "metrics = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nAnalyzing {name}...\")\n",
    "    \n",
    "    # Measure performance metrics\n",
    "    metrics[name] = measure_model_metrics(model, X_test_reshaped)\n",
    "    \n",
    "    # Get model summary\n",
    "    metrics[name]['summary'] = get_model_summary(model, metrics[name])\n",
    "    \n",
    "    # Find optimal batch size\n",
    "    metrics[name]['optimal_batch'] = optimize_batch_size(model, X_test_reshaped)\n",
    "    \n",
    "    # Calculate predictions and accuracy\n",
    "    y_pred = np.argmax(model.predict(X_test_reshaped), axis=1)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    metrics[name]['accuracy'] = accuracy\n",
    "    \n",
    "    # Calculate efficiency score\n",
    "    metrics[name]['efficiency_score'] = calculate_efficiency_score(\n",
    "        accuracy,\n",
    "        metrics[name]['model_size'],\n",
    "        metrics[name]['inference_time']['mean']\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Efficiency Score: {metrics[name]['efficiency_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot model sizes\n",
    "sizes = {name: m['model_size'] for name, m in metrics.items()}\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(sizes.keys(), sizes.values())\n",
    "\n",
    "# Color hybrid models differently\n",
    "for i, bar in enumerate(bars):\n",
    "    if any(x in list(sizes.keys())[i] for x in ['CNN-LSTM', 'CNN-BiLSTM']):\n",
    "        bar.set_color('orange')\n",
    "\n",
    "plt.title('Model Size Comparison')\n",
    "plt.ylabel('Size (KB)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, v in enumerate(sizes.values()):\n",
    "    plt.text(i, v, f'{v:.1f}KB', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Inference Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot inference times\n",
    "times = {name: m['inference_time']['mean'] for name, m in metrics.items()}\n",
    "std_times = {name: m['inference_time']['std'] for name, m in metrics.items()}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(times.keys(), times.values(),\n",
    "               yerr=list(std_times.values()),\n",
    "               capsize=5)\n",
    "\n",
    "# Color hybrid models\n",
    "for i, bar in enumerate(bars):\n",
    "    if any(x in list(times.keys())[i] for x in ['CNN-LSTM', 'CNN-BiLSTM']):\n",
    "        bar.set_color('orange')\n",
    "\n",
    "plt.title('Model Inference Time Comparison')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, v in enumerate(times.values()):\n",
    "    plt.text(i, v, f'{v:.1f}ms', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for name, m in metrics.items():\n",
    "    data = {\n",
    "        'Model': name,\n",
    "        'Accuracy': m['accuracy'],\n",
    "        'Size (KB)': m['model_size'],\n",
    "        'Inference Time (ms)': m['inference_time']['mean'],\n",
    "        'Memory Usage (MB)': m['memory_usage'],\n",
    "        'Parameters': m['summary']['parameters'],\n",
    "        'Efficiency Score': m['efficiency_score']\n",
    "    }\n",
    "    comparison_data.append(data)\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Plot efficiency scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(df['Model'], df['Efficiency Score'])\n",
    "\n",
    "# Color hybrid models\n",
    "for i, bar in enumerate(bars):\n",
    "    if any(x in df['Model'].iloc[i] for x in ['CNN-LSTM', 'CNN-BiLSTM']):\n",
    "        bar.set_color('orange')\n",
    "\n",
    "plt.title('Model Efficiency Comparison')\n",
    "plt.ylabel('Efficiency Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, v in enumerate(df['Efficiency Score']):\n",
    "    plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save metrics\n",
    "with open('results/model_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "\n",
    "# Save comparison DataFrame\n",
    "df.to_csv('results/model_comparison_metrics.csv', index=False)\n",
    "\n",
    "# Print final recommendations\n",
    "print(\"\\nModel Rankings by Efficiency:\")\n",
    "rankings = df.sort_values('Efficiency Score', ascending=False)\n",
    "print(rankings[['Model', 'Efficiency Score', 'Accuracy', 'Size (KB)', 'Inference Time (ms)']]\n",
    "      .to_string(index=False))\n",
    "\n",
    "best_model = rankings.iloc[0]['Model']\n",
    "print(f\"\\nBest performing model (efficiency-wise): {best_model}\")\n",
    "\n",
    "# Compare hybrid models\n",
    "hybrid_models = df[df['Model'].str.contains('CNN-')]\n",
    "print(\"\\nHybrid Model Performance:\")\n",
    "print(hybrid_models[['Model', 'Efficiency Score', 'Accuracy', 'Size (KB)']]\n",
    "      .to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}