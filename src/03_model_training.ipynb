{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook handles the training of all models with performance monitoring:\n",
    "1. Original models (1DCNN, RNN, DNN, LSTM, BiLSTM)\n",
    "2. Hybrid models (CNN-LSTM, CNN-BiLSTM)\n",
    "3. Memory and inference time tracking\n",
    "4. Training metrics logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psutil\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessed data\n",
    "X_train = np.load('preprocessed/X_train.npy')\n",
    "X_test = np.load('preprocessed/X_test.npy')\n",
    "y_train = np.load('preprocessed/y_train.npy')\n",
    "y_test = np.load('preprocessed/y_test.npy')\n",
    "\n",
    "# Reshape data for deep learning models\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Load model functions\n",
    "with open('models/model_functions.pkl', 'rb') as f:\n",
    "    model_functions = pickle.load(f)\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(f\"X_train: {X_train_reshaped.shape}\")\n",
    "print(f\"X_test: {X_test_reshaped.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Monitoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def measure_inference_time(model, X_test, batch_size=32, num_runs=100):\n",
    "    \"\"\"Measure average inference time\"\"\"\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        model.predict(X_test[:batch_size], verbose=0)\n",
    "        times.append(time.time() - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to milliseconds\n",
    "    std_time = np.std(times) * 1000\n",
    "    return avg_time, std_time\n",
    "\n",
    "def get_model_memory_usage(model):\n",
    "    \"\"\"Get model memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    baseline = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    # Force some predictions to load model into memory\n",
    "    model.predict(X_test_reshaped[:1], verbose=0)\n",
    "    \n",
    "    after_load = process.memory_info().rss / 1024 / 1024\n",
    "    return after_load - baseline\n",
    "\n",
    "def log_training_metrics(history, model_name):\n",
    "    \"\"\"Log training metrics to file\"\"\"\n",
    "    with open(f'logs/{model_name}_training.log', 'w') as f:\n",
    "        f.write(f\"Training metrics for {model_name}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        for metric, values in history.history.items():\n",
    "            f.write(f\"\\n{metric}:\\n\")\n",
    "            for epoch, value in enumerate(values):\n",
    "                f.write(f\"Epoch {epoch+1}: {value:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "results = {}\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "\n",
    "# Clear any existing models\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for name, create_model in model_functions.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model()\n",
    "    \n",
    "    # Log initial memory usage\n",
    "    initial_memory = get_model_memory_usage(model)\n",
    "    print(f\"Initial memory usage: {initial_memory:.2f} MB\")\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=0.0001\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=f'logs/{name}',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_reshaped,\n",
    "        y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Measure final memory usage\n",
    "    final_memory = get_model_memory_usage(model)\n",
    "    print(f\"Final memory usage: {final_memory:.2f} MB\")\n",
    "    \n",
    "    # Measure inference time\n",
    "    avg_time, std_time = measure_inference_time(model, X_test_reshaped)\n",
    "    print(f\"Average inference time: {avg_time:.2f} Â± {std_time:.2f} ms\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = np.argmax(model.predict(X_test_reshaped, batch_size=BATCH_SIZE), axis=1)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'history': history.history,\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'memory_usage': final_memory,\n",
    "        'inference_time': avg_time,\n",
    "        'inference_std': std_time\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Log training metrics\n",
    "    log_training_metrics(history, name)\n",
    "    \n",
    "    # Save model\n",
    "    model.save(f'models/{name}.h5')\n",
    "    \n",
    "    # Clear session for next model\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Model': name,\n",
    "        'Accuracy': res['accuracy'],\n",
    "        'Memory (MB)': res['memory_usage'],\n",
    "        'Inference (ms)': res['inference_time'],\n",
    "        'Parameters': res['model'].count_params()\n",
    "    }\n",
    "    for name, res in results.items()\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Plot comparisons\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.bar(df_comparison['Model'], df_comparison['Accuracy'])\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xticklabels(df_comparison['Model'], rotation=45)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "\n",
    "# Memory usage comparison\n",
    "ax2.bar(df_comparison['Model'], df_comparison['Memory (MB)'])\n",
    "ax2.set_title('Memory Usage')\n",
    "ax2.set_xticklabels(df_comparison['Model'], rotation=45)\n",
    "ax2.set_ylabel('Memory (MB)')\n",
    "\n",
    "# Inference time comparison\n",
    "ax3.bar(df_comparison['Model'], df_comparison['Inference (ms)'])\n",
    "ax3.set_title('Inference Time')\n",
    "ax3.set_xticklabels(df_comparison['Model'], rotation=45)\n",
    "ax3.set_ylabel('Time (ms)')\n",
    "\n",
    "# Parameter count comparison\n",
    "ax4.bar(df_comparison['Model'], df_comparison['Parameters'])\n",
    "ax4.set_title('Model Parameters')\n",
    "ax4.set_xticklabels(df_comparison['Model'], rotation=45)\n",
    "ax4.set_ylabel('Number of Parameters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "with open('results/training_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "df_comparison.to_csv('results/model_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}