{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Analysis with GPU Acceleration\n",
    "\n",
    "This notebook implements the air quality analysis with optimizations for NVIDIA RTX GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Flatten, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"\\nGPU memory growth enabled\")\n",
    "        \n",
    "        # Set TensorFlow to use the first GPU\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        \n",
    "        # Log GPU information\n",
    "        print(\"\\nGPU Information:\")\n",
    "        print(f\"Device: {tf.config.get_visible_devices('GPU')[0].device_type}\")\n",
    "        print(f\"Name: {tf.test.gpu_device_name()}\")\n",
    "        \n",
    "        # Test GPU with a simple matrix multiplication\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(\"\\nMatrix multiplication test on GPU successful\")\n",
    "            print(\"Result:\", c.numpy())\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found. Running on CPU.\")\n",
    "\n",
    "# Enable mixed precision for better performance\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(\"\\nCompute dtype:\", policy.compute_dtype)\n",
    "print(\"Variable dtype:\", policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data.csv')\n",
    "print(f\"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Function to categorize air quality\n",
    "def categorize_air_quality(value):\n",
    "    if 0 <= value <= 50:\n",
    "        return 'Good'\n",
    "    elif 51 <= value <= 100:\n",
    "        return 'Moderate'\n",
    "    elif 101 <= value <= 150:\n",
    "        return 'Unhealthy for Sensitive'\n",
    "    elif 151 <= value <= 200:\n",
    "        return 'Unhealthy'\n",
    "    elif 201 <= value <= 400:\n",
    "        return 'Hazardous'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Create categorical labels\n",
    "df['air_quality_category'] = df['Air Quality'].apply(categorize_air_quality)\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['air_quality_category'].value_counts())\n",
    "\n",
    "# Filter out classes with less than 2 samples\n",
    "min_samples = 2\n",
    "class_dist = df['air_quality_category'].value_counts()\n",
    "valid_classes = class_dist[class_dist >= min_samples].index\n",
    "df_filtered = df[df['air_quality_category'].isin(valid_classes)]\n",
    "\n",
    "print(f\"\\nFiltered data shape: {df_filtered.shape}\")\n",
    "print(\"\\nNew class distribution:\")\n",
    "print(df_filtered['air_quality_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle outliers using IQR method and replace with median\n",
    "def handle_outliers(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        median_val = df[col].median()\n",
    "        \n",
    "        # Plot before outlier handling\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.boxplot(df[col])\n",
    "        plt.title(f'{col} - Before Outlier Handling')\n",
    "        \n",
    "        # Replace outliers with median\n",
    "        df_clean.loc[df_clean[col] < lower_bound, col] = median_val\n",
    "        df_clean.loc[df_clean[col] > upper_bound, col] = median_val\n",
    "        \n",
    "        # Plot after outlier handling\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(df_clean[col])\n",
    "        plt.title(f'{col} - After Outlier Handling')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return df_clean\n",
    "\n",
    "# Define features for the models\n",
    "features = ['CO2', 'TVOC', 'PM10', 'PM2.5', 'CO', 'LDR', 'O3', 'Temp', 'Hum']\n",
    "X = df_filtered[features].copy()\n",
    "\n",
    "# Handle outliers\n",
    "X = handle_outliers(X, features)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Prepare classification target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_filtered['air_quality_category'])\n",
    "\n",
    "# Split data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE for class balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n",
    "\n",
    "# Reshape data for deep learning models\n",
    "X_train_reshaped = X_train_balanced.reshape((X_train_balanced.shape[0], X_train_balanced.shape[1], 1))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions with GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "num_features = len(features)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Configuration for better GPU performance\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def create_1dcnn():\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = Sequential([\n",
    "            Conv1D(128, 2, activation='relu', input_shape=(num_features, 1)),\n",
    "            Conv1D(64, 2, activation='relu'),\n",
    "            Conv1D(32, 2, activation='relu'),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "def create_rnn():\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = Sequential([\n",
    "            tf.keras.layers.SimpleRNN(128, input_shape=(num_features, 1)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "def create_dnn():\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=(num_features, 1)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "def create_lstm():\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = Sequential([\n",
    "            LSTM(128, input_shape=(num_features, 1)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "def create_bilstm():\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(128), input_shape=(num_features, 1)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clear GPU memory before training\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "models = {\n",
    "    '1DCNN': create_1dcnn(),\n",
    "    'RNN': create_rnn(),\n",
    "    'DNN': create_dnn(),\n",
    "    'LSTM': create_lstm(),\n",
    "    'BiLSTM': create_bilstm()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(optimizer=optimizer,\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "        \n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=f'./logs/{name}',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_reshaped,\n",
    "            y_train_balanced,\n",
    "            validation_split=0.2,\n",
    "            epochs=50,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[early_stop, reduce_lr, tensorboard],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = np.argmax(model.predict(X_test_reshaped, batch_size=BATCH_SIZE), axis=1)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'history': history.history,\n",
    "            'model': model,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name} Results:\")\n",
    "        print(f\"Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "        \n",
    "        # Clear GPU memory after each model\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def optimize_and_save_model(model, model_name):\n",
    "    \"\"\"\n",
    "    Optimize and save model with proper TFLite conversion settings.\n",
    "    \"\"\"\n",
    "    print(f\"\\nOptimizing {model_name}...\")\n",
    "    \n",
    "    # Save original model\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    model_path = f'models/{model_name}.h5'\n",
    "    model.save(model_path)\n",
    "    \n",
    "    # Configure TFLite converter\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    # Enable experimental features for tensor list ops\n",
    "    converter.experimental_enable_resource_variables = True\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    \n",
    "    # Set optimization options\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    \n",
    "    # Convert model\n",
    "    try:\n",
    "        quantized_model = converter.convert()\n",
    "        \n",
    "        # Save quantized model\n",
    "        quantized_path = f'models/{model_name}_quantized.tflite'\n",
    "        with open(quantized_path, 'wb') as f:\n",
    "            f.write(quantized_model)\n",
    "            \n",
    "        # Print size comparison\n",
    "        original_size = os.path.getsize(model_path) / 1024  # KB\n",
    "        quantized_size = os.path.getsize(quantized_path) / 1024  # KB\n",
    "        \n",
    "        print(f\"\\nModel Size Comparison:\")\n",
    "        print(f\"Original: {original_size:.2f} KB\")\n",
    "        print(f\"Quantized: {quantized_size:.2f} KB\")\n",
    "        print(f\"Size reduction: {((original_size - quantized_size) / original_size * 100):.2f}%\")\n",
    "        \n",
    "        return True, quantized_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during model optimization: {str(e)}\")\n",
    "        print(\"Falling back to original model only\")\n",
    "        return False, model_path\n",
    "\n",
    "# Get best performing model\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# Optimize and save the model\n",
    "success, model_path = optimize_and_save_model(best_model, f\"best_model_{best_model_name}\")\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nOptimized model saved to: {model_path}\")\n",
    "else:\n",
    "    print(f\"\\nOriginal model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history for all models\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for name in results:\n",
    "    plt.plot(results[name]['history']['accuracy'], label=f'{name}')\n",
    "plt.title('Model Accuracy During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for name in results:\n",
    "    plt.plot(results[name]['history']['loss'], label=f'{name}')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix for best model\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, results[best_model_name]['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
