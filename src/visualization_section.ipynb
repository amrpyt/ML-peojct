{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Model Performance Comparison\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Classification Models:\")\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nRegression Models:\")\n",
    "print(\"\\nTemperature Prediction:\")\n",
    "for metric, value in metrics['Temperature'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nHumidity Prediction:\")\n",
    "for metric, value in metrics['Humidity'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# 2. Training History Visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for name in results:\n",
    "    plt.plot(results[name]['history']['accuracy'], label=f'{name}')\n",
    "plt.title('Model Accuracy During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for name in results:\n",
    "    plt.plot(results[name]['history']['loss'], label=f'{name}')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Best Model Performance Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "cm = confusion_matrix(y_test, results[best_model_name]['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(f'Confusion Matrix - {best_model_name} (Best Model)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# 4. Model Size Comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "sizes = {\n",
    "    'Original': original_size,\n",
    "    'Pruned': os.path.getsize(f'models/best_model_{best_model_name}_pruned.h5') / 1024,\n",
    "    'Quantized': os.path.getsize(f'models/best_model_{best_model_name}_quantized.tflite') / 1024\n",
    "}\n",
    "\n",
    "plt.bar(sizes.keys(), sizes.values())\n",
    "plt.title('Model Size Comparison')\n",
    "plt.ylabel('Size (KB)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, v in enumerate(sizes.values()):\n",
    "    plt.text(i, v + 1, f'{v:.1f} KB', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Final Report\n",
    "print(\"\\nFinal Report\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(\"\\nOptimization Results:\")\n",
    "print(f\"Original Size: {original_size:.1f} KB\")\n",
    "print(f\"Final Size: {sizes['Quantized']:.1f} KB\")\n",
    "print(f\"Size Reduction: {((original_size - sizes['Quantized']) / original_size * 100):.1f}%\")\n",
    "\n",
    "print(\"\\nTemperature Prediction Performance:\")\n",
    "print(f\"R² Score: {metrics['Temperature']['R²']:.4f}\")\n",
    "print(f\"RMSE: {metrics['Temperature']['RMSE']:.4f}\")\n",
    "\n",
    "print(\"\\nHumidity Prediction Performance:\")\n",
    "print(f\"R² Score: {metrics['Humidity']['R²']:.4f}\")\n",
    "print(f\"RMSE: {metrics['Humidity']['RMSE']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}