{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 74\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_original.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Get best performing model\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m best_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     75\u001b[0m best_model \u001b[38;5;241m=\u001b[39m results[best_model_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest performing model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "def optimize_model(model, model_name):\n",
    "    \"\"\"Optimize model using TensorFlow's built-in capabilities\"\"\"\n",
    "    print(f\"\\nOptimizing {model_name}...\")\n",
    "\n",
    "    # Create models directory\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    base_path = f\"models/{model_name}\"\n",
    "\n",
    "    # Save original model\n",
    "    model.save(f\"{base_path}_original.h5\")\n",
    "    original_size = os.path.getsize(f\"{base_path}_original.h5\") / 1024  # KB\n",
    "\n",
    "    try:\n",
    "        # Configure TFLite converter\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "        # Enable optimizations\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "        # Enable float16 quantization\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "        # Add support for TF ops\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS,\n",
    "        ]\n",
    "\n",
    "        # Convert model\n",
    "        print(\"Converting to TFLite format...\")\n",
    "        quantized_model = converter.convert()\n",
    "\n",
    "        # Save quantized model\n",
    "        quantized_path = f\"{base_path}_quantized.tflite\"\n",
    "        with open(quantized_path, \"wb\") as f:\n",
    "            f.write(quantized_model)\n",
    "\n",
    "        # Compare model sizes\n",
    "        sizes = {\n",
    "            \"Original\": original_size,\n",
    "            \"Quantized\": os.path.getsize(quantized_path) / 1024,\n",
    "        }\n",
    "\n",
    "        # Plot size comparison\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(sizes.keys(), sizes.values())\n",
    "        plt.title(\"Model Size Comparison\")\n",
    "        plt.ylabel(\"Size (KB)\")\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        for i, v in enumerate(sizes.values()):\n",
    "            plt.text(i, v + 1, f\"{v:.1f} KB\", ha=\"center\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print size comparison\n",
    "        print(\"\\nModel Size Comparison:\")\n",
    "        for name, size in sizes.items():\n",
    "            print(f\"{name}: {size:.2f} KB\")\n",
    "        print(\n",
    "            f\"Size reduction: {((original_size - sizes['Quantized']) / original_size * 100):.2f}%\"\n",
    "        )\n",
    "\n",
    "        return True, quantized_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during optimization: {str(e)}\")\n",
    "        print(\"Falling back to original model\")\n",
    "        return False, f\"{base_path}_original.h5\"\n",
    "\n",
    "\n",
    "# Get best performing model\n",
    "best_model_name = max(results.items(), key=lambda x: x[1][\"accuracy\"])[0]\n",
    "best_model = results[best_model_name][\"model\"]\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# Optimize the model\n",
    "success, model_path = optimize_model(best_model, f\"best_model_{best_model_name}\")\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nOptimized model saved to: {model_path}\")\n",
    "else:\n",
    "    print(f\"\\nOriginal model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
