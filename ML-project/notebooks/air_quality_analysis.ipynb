{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def optimize_model(model, model_name):\n",
    "    \"\"\"Optimize model using TensorFlow's built-in capabilities\"\"\"\n",
    "    print(f\"\\nOptimizing {model_name}...\")\n",
    "    \n",
    "    # Create models directory\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    base_path = f'models/{model_name}'\n",
    "    \n",
    "    # Save original model\n",
    "    model.save(f'{base_path}_original.h5')\n",
    "    original_size = os.path.getsize(f'{base_path}_original.h5') / 1024  # KB\n",
    "    \n",
    "    try:\n",
    "        # Configure TFLite converter\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        \n",
    "        # Enable optimizations\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "        # Enable float16 quantization\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "        \n",
    "        # Add support for TF ops\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        \n",
    "        # Convert model\n",
    "        print(\"Converting to TFLite format...\")\n",
    "        quantized_model = converter.convert()\n",
    "        \n",
    "        # Save quantized model\n",
    "        quantized_path = f'{base_path}_quantized.tflite'\n",
    "        with open(quantized_path, 'wb') as f:\n",
    "            f.write(quantized_model)\n",
    "            \n",
    "        # Compare model sizes\n",
    "        sizes = {\n",
    "            'Original': original_size,\n",
    "            'Quantized': os.path.getsize(quantized_path) / 1024\n",
    "        }\n",
    "        \n",
    "        # Plot size comparison\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(sizes.keys(), sizes.values())\n",
    "        plt.title('Model Size Comparison')\n",
    "        plt.ylabel('Size (KB)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        for i, v in enumerate(sizes.values()):\n",
    "            plt.text(i, v + 1, f'{v:.1f} KB', ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print size comparison\n",
    "        print(\"\\nModel Size Comparison:\")\n",
    "        for name, size in sizes.items():\n",
    "            print(f\"{name}: {size:.2f} KB\")\n",
    "        print(f\"Size reduction: {((original_size - sizes['Quantized']) / original_size * 100):.2f}%\")\n",
    "        \n",
    "        return True, quantized_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during optimization: {str(e)}\")\n",
    "        print(\"Falling back to original model\")\n",
    "        return False, f'{base_path}_original.h5'\n",
    "\n",
    "# Get best performing model\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# Optimize the model\n",
    "success, model_path = optimize_model(best_model, f\"best_model_{best_model_name}\")\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nOptimized model saved to: {model_path}\")\n",
    "else:\n",
    "    print(f\"\\nOriginal model saved to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
