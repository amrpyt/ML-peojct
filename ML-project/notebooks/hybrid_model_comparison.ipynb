{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model Performance Comparison\n",
    "\n",
    "This notebook compares the performance of:\n",
    "1. Original CNN-BiLSTM model\n",
    "2. Improved CNN-BiLSTM architecture\n",
    "3. Analysis of improvements in accuracy and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load test data\n",
    "X_test = np.load('preprocessed/X_test.npy')\n",
    "y_test = np.load('preprocessed/y_test.npy')\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Load both models\n",
    "original_model = load_model('models/CNN-BiLSTM.h5')\n",
    "improved_model = load_model('models/improved_cnn_bilstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, name):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    # Predictions\n",
    "    y_pred = np.argmax(model.predict(X_test_reshaped), axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Original Model Performance:\")\n",
    "original_metrics = evaluate_model(original_model, \"Original CNN-BiLSTM\")\n",
    "\n",
    "print(\"\\nImproved Model Performance:\")\n",
    "improved_metrics = evaluate_model(improved_model, \"Improved CNN-BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare model architectures\n",
    "print(\"Model Architecture Comparison:\")\n",
    "print(\"\\nOriginal Model:\")\n",
    "original_model.summary()\n",
    "\n",
    "print(\"\\nImproved Model:\")\n",
    "improved_model.summary()\n",
    "\n",
    "# Compare parameters and size\n",
    "original_params = original_model.count_params()\n",
    "improved_params = improved_model.count_params()\n",
    "\n",
    "print(f\"\\nParameter Count:\")\n",
    "print(f\"Original Model: {original_params:,}\")\n",
    "print(f\"Improved Model: {improved_params:,}\")\n",
    "print(f\"Difference: {improved_params - original_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot accuracy comparison\n",
    "metrics = {\n",
    "    'Model': ['Original', 'Improved'],\n",
    "    'Accuracy': [original_metrics['accuracy'], improved_metrics['accuracy']],\n",
    "    'Macro F1': [original_metrics['macro avg']['f1-score'], \n",
    "                 improved_metrics['macro avg']['f1-score']]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(metrics['Model']))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, metrics['Accuracy'], width, label='Accuracy')\n",
    "plt.bar(x + width/2, metrics['Macro F1'], width, label='Macro F1')\n",
    "\n",
    "plt.xlabel('Model Version')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, metrics['Model'])\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i in x:\n",
    "    plt.text(i - width/2, metrics['Accuracy'][i], f\"{metrics['Accuracy'][i]:.4f}\", \n",
    "             ha='center', va='bottom')\n",
    "    plt.text(i + width/2, metrics['Macro F1'][i], f\"{metrics['Macro F1'][i]:.4f}\", \n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Print detailed comparison\n",
    "print(\"Detailed Performance Comparison:\")\n",
    "print(\"\\nOriginal Model:\")\n",
    "print(f\"Accuracy: {original_metrics['accuracy']:.4f}\")\n",
    "print(f\"Macro F1: {original_metrics['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted F1: {original_metrics['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "print(\"\\nImproved Model:\")\n",
    "print(f\"Accuracy: {improved_metrics['accuracy']:.4f}\")\n",
    "print(f\"Macro F1: {improved_metrics['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted F1: {improved_metrics['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "acc_improvement = (improved_metrics['accuracy'] - original_metrics['accuracy']) * 100\n",
    "f1_improvement = (improved_metrics['macro avg']['f1-score'] - \n",
    "                 original_metrics['macro avg']['f1-score']) * 100\n",
    "\n",
    "print(f\"\\nImprovements:\")\n",
    "print(f\"Accuracy: +{acc_improvement:.2f}%\")\n",
    "print(f\"Macro F1: +{f1_improvement:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}