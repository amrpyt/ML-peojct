{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization\n",
    "\n",
    "This notebook focuses on making models lightweight and efficient using:\n",
    "1. Model pruning\n",
    "2. Weight quantization\n",
    "3. Tensor compression\n",
    "4. Architecture optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load training results\n",
    "with open('results/training_results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Compare model sizes and accuracies\n",
    "print(\"Model Performance Summary:\")\n",
    "print(\"-\" * 50)\n",
    "for name, result in results.items():\n",
    "    model = result['model']\n",
    "    params = model.count_params()\n",
    "    accuracy = result['accuracy']\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_model_size(model, name):\n",
    "    \"\"\"Analyze model size and complexity\"\"\"\n",
    "    # Save original model\n",
    "    base_path = f'models/{name}'\n",
    "    model.save(f'{base_path}_original.h5')\n",
    "    original_size = os.path.getsize(f'{base_path}_original.h5') / 1024  # KB\n",
    "    \n",
    "    # Get parameter count by layer\n",
    "    layer_params = [(layer.name, layer.count_params()) for layer in model.layers]\n",
    "    \n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Total size: {original_size:.2f} KB\")\n",
    "    print(\"\\nParameters by layer:\")\n",
    "    for layer_name, params in layer_params:\n",
    "        print(f\"{layer_name}: {params:,}\")\n",
    "    \n",
    "    return original_size, layer_params\n",
    "\n",
    "# Analyze all models\n",
    "size_analysis = {}\n",
    "for name, result in results.items():\n",
    "    size_analysis[name] = analyze_model_size(result['model'], name)\n",
    "\n",
    "# Plot model sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(size_analysis.keys())\n",
    "sizes = [analysis[0] for analysis in size_analysis.values()]\n",
    "\n",
    "plt.bar(names, sizes)\n",
    "plt.title('Model Size Comparison')\n",
    "plt.ylabel('Size (KB)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, v in enumerate(sizes):\n",
    "    plt.text(i, v, f'{v:.1f}KB', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def quantize_model(model):\n",
    "    \"\"\"Quantize model to reduce size\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    return converter.convert()\n",
    "\n",
    "def compress_weights(model):\n",
    "    \"\"\"Apply weight compression techniques\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            # Apply 16-bit floating point\n",
    "            weights = layer.get_weights()\n",
    "            weights = [w.astype(np.float16) for w in weights]\n",
    "            layer.set_weights(weights)\n",
    "    return model\n",
    "\n",
    "def optimize_architecture(model):\n",
    "    \"\"\"Optimize model architecture\"\"\"\n",
    "    # Remove unnecessary layers\n",
    "    optimized_model = tf.keras.models.Sequential()\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dropout):\n",
    "            # Adjust dropout rate\n",
    "            optimized_model.add(tf.keras.layers.Dropout(0.1))\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            # Reduce dense layer size\n",
    "            units = layer.units\n",
    "            if layer != model.layers[-1]:  # Don't modify output layer\n",
    "                units = units // 2\n",
    "            optimized_model.add(tf.keras.layers.Dense(units, activation=layer.activation))\n",
    "        else:\n",
    "            optimized_model.add(layer)\n",
    "    \n",
    "    return optimized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def optimize_model(model, name, X_test, y_test):\n",
    "    \"\"\"Apply all optimization techniques and evaluate results\"\"\"\n",
    "    print(f\"\\nOptimizing {name}...\")\n",
    "    \n",
    "    # Save original metrics\n",
    "    original_size = os.path.getsize(f'models/{name}_original.h5') / 1024\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    original_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results = {\n",
    "        'Original': {\n",
    "            'size': original_size,\n",
    "            'accuracy': original_accuracy\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 1. Architecture optimization\n",
    "        arch_optimized = optimize_architecture(model)\n",
    "        arch_optimized.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        arch_optimized.fit(X_test, y_test, epochs=5, verbose=0)  # Quick fine-tuning\n",
    "        \n",
    "        arch_optimized.save(f'models/{name}_arch_opt.h5')\n",
    "        y_pred = np.argmax(arch_optimized.predict(X_test), axis=1)\n",
    "        \n",
    "        results['Architecture Optimized'] = {\n",
    "            'size': os.path.getsize(f'models/{name}_arch_opt.h5') / 1024,\n",
    "            'accuracy': accuracy_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # 2. Weight compression\n",
    "        compressed = compress_weights(arch_optimized)\n",
    "        compressed.save(f'models/{name}_compressed.h5')\n",
    "        y_pred = np.argmax(compressed.predict(X_test), axis=1)\n",
    "        \n",
    "        results['Compressed'] = {\n",
    "            'size': os.path.getsize(f'models/{name}_compressed.h5') / 1024,\n",
    "            'accuracy': accuracy_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # 3. Quantization\n",
    "        quantized = quantize_model(compressed)\n",
    "        with open(f'models/{name}_quantized.tflite', 'wb') as f:\n",
    "            f.write(quantized)\n",
    "            \n",
    "        results['Quantized'] = {\n",
    "            'size': os.path.getsize(f'models/{name}_quantized.tflite') / 1024,\n",
    "            'accuracy': results['Compressed']['accuracy']  # Same as compressed model\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nOptimization Results for {name}:\")\n",
    "        for version, metrics in results.items():\n",
    "            print(f\"\\n{version}:\")\n",
    "            print(f\"Size: {metrics['size']:.2f} KB\")\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        return True, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during optimization: {str(e)}\")\n",
    "        return False, results\n",
    "\n",
    "# Load test data\n",
    "X_test = np.load('preprocessed/X_test.npy')\n",
    "y_test = np.load('preprocessed/y_test.npy')\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Optimize all models\n",
    "optimization_results = {}\n",
    "for name, result in results.items():\n",
    "    success, metrics = optimize_model(result['model'], name, X_test_reshaped, y_test)\n",
    "    if success:\n",
    "        optimization_results[name] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_optimization_results(results):\n",
    "    \"\"\"Plot size and accuracy comparisons\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Size comparison\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, metrics in results.items():\n",
    "        versions = list(metrics.keys())\n",
    "        sizes = [metrics[v]['size'] for v in versions]\n",
    "        x = range(len(versions))\n",
    "        plt.plot(x, sizes, marker='o', label=name)\n",
    "    \n",
    "    plt.title('Model Size Comparison')\n",
    "    plt.xlabel('Optimization Stage')\n",
    "    plt.ylabel('Size (KB)')\n",
    "    plt.xticks(range(len(versions)), versions, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, metrics in results.items():\n",
    "        versions = list(metrics.keys())\n",
    "        accuracies = [metrics[v]['accuracy'] for v in versions]\n",
    "        x = range(len(versions))\n",
    "        plt.plot(x, accuracies, marker='o', label=name)\n",
    "    \n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xlabel('Optimization Stage')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(range(len(versions)), versions, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "plot_optimization_results(optimization_results)\n",
    "\n",
    "# Save optimization results\n",
    "with open('results/optimization_results.pkl', 'wb') as f:\n",
    "    pickle.dump(optimization_results, f)\n",
    "\n",
    "print(\"\\nOptimization results saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}