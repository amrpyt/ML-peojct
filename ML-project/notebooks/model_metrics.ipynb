{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Metrics\n",
    "\n",
    "This notebook analyzes model performance with focus on efficiency metrics:\n",
    "1. Memory usage\n",
    "2. Inference time\n",
    "3. Model size\n",
    "4. Efficiency scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\style\\core.py:137\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     style \u001b[38;5;241m=\u001b[39m \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\__init__.py:879\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    878\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 879\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_file_or_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfd\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\__init__.py:856\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    855\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[1;32m--> 856\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     measure_model_metrics,\n\u001b[0;32m      9\u001b[0m     calculate_efficiency_score,\n\u001b[0;32m     10\u001b[0m     get_model_summary,\n\u001b[0;32m     11\u001b[0m     optimize_batch_size,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Set style for better visualizations\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseaborn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_palette(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhusl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\style\\core.py:139\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    137\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    143\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import (\n",
    "    measure_model_metrics,\n",
    "    calculate_efficiency_score,\n",
    "    get_model_summary,\n",
    "    optimize_batch_size,\n",
    ")\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use(\"seaborn\")\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_test = np.load(\"preprocessed/X_test.npy\")\n",
    "y_test = np.load(\"preprocessed/y_test.npy\")\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Load all models\n",
    "models = {}\n",
    "for model_file in os.listdir(\"models\"):\n",
    "    if model_file.endswith(\".h5\"):\n",
    "        name = model_file[:-3]\n",
    "        models[name] = tf.keras.models.load_model(f\"models/{model_file}\")\n",
    "\n",
    "print(f\"Loaded {len(models)} models:\")\n",
    "for name in models.keys():\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measure Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure metrics for each model\n",
    "metrics = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nAnalyzing {name}...\")\n",
    "    \n",
    "    # Measure performance metrics\n",
    "    metrics[name] = measure_model_metrics(model, X_test_reshaped)\n",
    "    \n",
    "    # Get model summary\n",
    "    metrics[name]['summary'] = get_model_summary(model, metrics[name])\n",
    "    \n",
    "    # Find optimal batch size\n",
    "    metrics[name]['optimal_batch'] = optimize_batch_size(model, X_test_reshaped)\n",
    "    \n",
    "    # Calculate predictions and accuracy\n",
    "    y_pred = np.argmax(model.predict(X_test_reshaped), axis=1)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    metrics[name]['accuracy'] = accuracy\n",
    "    \n",
    "    # Calculate efficiency score\n",
    "    metrics[name]['efficiency_score'] = calculate_efficiency_score(\n",
    "        accuracy,\n",
    "        metrics[name]['model_size'],\n",
    "        metrics[name]['inference_time']['mean']\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Efficiency Score: {metrics[name]['efficiency_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model sizes\n",
    "sizes = {name: m['model_size'] for name, m in metrics.items()}\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(sizes.keys(), sizes.values())\n",
    "\n",
    "# Color hybrid models differently\n",
    "for i, bar in enumerate(bars):\n",
    "    if any(x in list(sizes.keys())[i] for x in ['CNN-LSTM', 'CNN-BiLSTM']):\n",
    "        bar.set_color('orange')\n",
    "\n",
    "plt.title('Model Size Comparison')\n",
    "plt.ylabel('Size (KB)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, v in enumerate(sizes.values()):\n",
    "    plt.text(i, v, f'{v:.1f}KB', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Inference Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inference times\n",
    "times = {name: m['inference_time']['mean'] for name, m in metrics.items()}\n",
    "std_times = {name: m['inference_time']['std'] for name, m in metrics.items()}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(times.keys(), times.values(),\n",
    "               yerr=list(std_times.values()),\n",
    "               capsize=5)\n",
    "\n",
    "# Color hybrid models\n",
    "for i, bar in enumerate(bars):\n",
    "    if any(x in list(times.keys())[i] for x in ['CNN-LSTM', 'CNN-BiLSTM']):\n",
    "        bar.set_color('orange')\n",
    "\n",
    "plt.title('Model Inference Time Comparison')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, v in enumerate(times.values()):\n",
    "    plt.text(i, v, f'{v:.1f}ms', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for name, m in metrics.items():\n",
    "    data = {\n",
    "        'Model': name,\n",
    "        'Accuracy': m['accuracy'],\n",
    "        'Size (KB)': m['model_size'],\n",
    "        'Inference Time (ms)': m['inference_time']['mean'],\n",
    "        'Memory Usage (MB)': m['memory_usage'],\n",
    "        'Parameters': m['summary']['parameters'],\n",
    "        'Efficiency Score': m['efficiency_score']\n",
    "    }\n",
    "    comparison_data.append(data)\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Plot efficiency scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(df['Model'], df['Efficiency Score'])\n",
    "\n",
    "# Color hybrid models\n",
    "for i, bar in enumerate(bars):\n",
    "    if any(x in df['Model'].iloc[i] for x in ['CNN-LSTM', 'CNN-BiLSTM']):\n",
    "        bar.set_color('orange')\n",
    "\n",
    "plt.title('Model Efficiency Comparison')\n",
    "plt.ylabel('Efficiency Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, v in enumerate(df['Efficiency Score']):\n",
    "    plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "with open('results/model_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "\n",
    "# Save comparison DataFrame\n",
    "df.to_csv('results/model_comparison_metrics.csv', index=False)\n",
    "\n",
    "# Print final recommendations\n",
    "print(\"\\nModel Rankings by Efficiency:\")\n",
    "rankings = df.sort_values('Efficiency Score', ascending=False)\n",
    "print(rankings[['Model', 'Efficiency Score', 'Accuracy', 'Size (KB)', 'Inference Time (ms)']]\n",
    "      .to_string(index=False))\n",
    "\n",
    "best_model = rankings.iloc[0]['Model']\n",
    "print(f\"\\nBest performing model (efficiency-wise): {best_model}\")\n",
    "\n",
    "# Compare hybrid models\n",
    "hybrid_models = df[df['Model'].str.contains('CNN-')]\n",
    "print(\"\\nHybrid Model Performance:\")\n",
    "print(hybrid_models[['Model', 'Efficiency Score', 'Accuracy', 'Size (KB)']]\n",
    "      .to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
